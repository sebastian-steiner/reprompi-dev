From 0c59c4237c70492641091996c47365f3875252b1 Mon Sep 17 00:00:00 2001
From: Sascha Hunold <sascha@hunoldscience.net>
Date: Thu, 1 Sep 2022 14:01:48 +0200
Subject: [PATCH 1/5] updated clock sync test programs

---
 docker/Dockerfile                             |  8 +++++
 .../clock_drift/measure_clock_drift_new.c     | 29 +++++++----------
 .../clock_drift/measure_real_clock_drift.c    | 32 +++++++++----------
 3 files changed, 35 insertions(+), 34 deletions(-)

diff --git a/docker/Dockerfile b/docker/Dockerfile
index 2fad45f..2efa472 100644
--- a/docker/Dockerfile
+++ b/docker/Dockerfile
@@ -1,5 +1,7 @@
 FROM debian:bullseye
 
+# Docker file for ReproMPI (dev)
+
 LABEL maintainer="Sascha Hunold <sascha@hunoldscience.net>"
 
 RUN apt-get clean \
@@ -19,6 +21,12 @@ RUN apt-get clean \
     mpich \
     build-essential \
     tmux \
+    flex \
+    python3 \
+    autoconf \
+    automake \
+    libtool \
+    libev-dev \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*
 
diff --git a/src/sanity_check/clock_drift/measure_clock_drift_new.c b/src/sanity_check/clock_drift/measure_clock_drift_new.c
index 76a399f..f0830d7 100644
--- a/src/sanity_check/clock_drift/measure_clock_drift_new.c
+++ b/src/sanity_check/clock_drift/measure_clock_drift_new.c
@@ -71,12 +71,7 @@ void print_help(char* testname) {
     MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
     if (my_rank == root_proc) {
 
-        if (strstr(testname, "measure_clock_drift") != 0) {
-            printf("\nUSAGE: %s [options] [steps]\n", testname);
-        }
-        else {
-            printf("\nUSAGE: %s [options]\n", testname);
-        }
+        printf("\nUSAGE: %s [options]\n", testname);
 
         printf("options:\n");
         printf("%-40s %-40s\n", "-h", "print this help");
@@ -422,7 +417,7 @@ int main(int argc, char* argv[]) {
 
 double SKaMPIClockOffset_measure_offset(MPI_Comm comm, int ref_rank, int client_rank, reprompib_sync_module_t *clock_sync) {
   // SKaMPI pingpongs
-  int i, other_global_id;
+  int i; //, other_global_id;
   double s_now, s_last, t_last, t_now;
   double td_min, td_max;
   double invalid_time = -1.0;
@@ -453,15 +448,15 @@ double SKaMPIClockOffset_measure_offset(MPI_Comm comm, int ref_rank, int client_
   // check whether we have ping_pong_min_time in our hash
   // if so, take it and use it (can stop after min_n_ping_pong rounds)
   // if not, we set ping_pong_min_time to -1.0 (then we need to do n_ping_pongs rounds)
-  int rank1;
-  int rank2;
-  if( ref_rank < client_rank ) {
-    rank1 = ref_rank;
-    rank2 = client_rank;
-  } else {
-    rank1 = client_rank;
-    rank2 = ref_rank;
-  }
+  //  int rank1;
+  //  int rank2;
+  //  if( ref_rank < client_rank ) {
+  //    rank1 = ref_rank;
+  //    rank2 = client_rank;
+  //  } else {
+  //    rank1 = client_rank;
+  //    rank2 = ref_rank;
+  //  }
 
   ping_pong_min_time = -1.0;
 
@@ -479,7 +474,7 @@ double SKaMPIClockOffset_measure_offset(MPI_Comm comm, int ref_rank, int client_
     td_max = t_last - s_last;
 
   } else {
-    other_global_id = ref_rank;
+    //other_global_id = ref_rank;
 
     MPI_Recv(&s_last, 1, MPI_DOUBLE, ref_rank, pp_tag, comm, &status);
     t_last = clock_sync->get_global_time(get_time());
diff --git a/src/sanity_check/clock_drift/measure_real_clock_drift.c b/src/sanity_check/clock_drift/measure_real_clock_drift.c
index 55d6cdc..a9075a9 100644
--- a/src/sanity_check/clock_drift/measure_real_clock_drift.c
+++ b/src/sanity_check/clock_drift/measure_real_clock_drift.c
@@ -119,7 +119,7 @@ void print_help(char* testname) {
 void init_parameters(reprompib_drift_test_opts_t* opts_p, char* name) {
   opts_p->n_rep    = 5;
   opts_p->nwaits  = 60;
-  opts_p->wait_ms = 500.0;
+  opts_p->wait_ms = 500;
   strcpy(opts_p->testname, name);
 }
 
@@ -202,7 +202,7 @@ int main(int argc, char* argv[]) {
     FILE* f;
 
     double cur_offset;
-    res_str_t *clock_offsets;
+    res_str_t *clock_offsets = NULL;
 
     int nrows;
     struct timespec ts;
@@ -229,10 +229,8 @@ int main(int argc, char* argv[]) {
 
     Number_ping_pongs = opts.n_rep;
 
-    ts.tv_sec  = 0;
-    ts.tv_nsec = opts.wait_ms * 1E6;
-
-
+    ts.tv_sec  = (long) opts.wait_ms / 1000;
+    ts.tv_nsec = (long) (opts.wait_ms % 1000) * 1E6;
 
     /* warm up */
     {
@@ -307,7 +305,7 @@ int main(int argc, char* argv[]) {
 
 double SKaMPIClockOffset_measure_offset(MPI_Comm comm, int ref_rank, int client_rank) {
   // SKaMPI pingpongs
-  int i, other_global_id;
+  int i; //, other_global_id;
   double s_now, s_last, t_last, t_now;
   double td_min, td_max;
   double invalid_time = -1.0;
@@ -338,15 +336,15 @@ double SKaMPIClockOffset_measure_offset(MPI_Comm comm, int ref_rank, int client_
   // check whether we have ping_pong_min_time in our hash
   // if so, take it and use it (can stop after min_n_ping_pong rounds)
   // if not, we set ping_pong_min_time to -1.0 (then we need to do n_ping_pongs rounds)
-  int rank1;
-  int rank2;
-  if( ref_rank < client_rank ) {
-    rank1 = ref_rank;
-    rank2 = client_rank;
-  } else {
-    rank1 = client_rank;
-    rank2 = ref_rank;
-  }
+  //  int rank1;
+  //  int rank2;
+  //  if( ref_rank < client_rank ) {
+  //    rank1 = ref_rank;
+  //    rank2 = client_rank;
+  //  } else {
+  //    rank1 = client_rank;
+  //    rank2 = ref_rank;
+  //  }
 
   ping_pong_min_time = -1.0;
 
@@ -364,7 +362,7 @@ double SKaMPIClockOffset_measure_offset(MPI_Comm comm, int ref_rank, int client_
     td_max = t_last - s_last;
 
   } else {
-    other_global_id = ref_rank;
+    //other_global_id = ref_rank;
 
     MPI_Recv(&s_last, 1, MPI_DOUBLE, ref_rank, pp_tag, comm, &status);
     t_last = get_time();
-- 
2.38.0


From 92e1ab5710cc7d26b4ed58a5b93c62a11aae6868 Mon Sep 17 00:00:00 2001
From: Sascha Hunold <sascha@hunoldscience.net>
Date: Thu, 1 Sep 2022 14:03:56 +0200
Subject: [PATCH 2/5] use PMPI_Wtime, so we can intercept MPI_Wtime

---
 src/reprompi_bench/sync/clock_sync/clocks/MPIClock.cpp | 2 +-
 src/reprompi_bench/sync/time_measurement.c             | 2 +-
 2 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/reprompi_bench/sync/clock_sync/clocks/MPIClock.cpp b/src/reprompi_bench/sync/clock_sync/clocks/MPIClock.cpp
index 1613c73..7aff0ba 100644
--- a/src/reprompi_bench/sync/clock_sync/clocks/MPIClock.cpp
+++ b/src/reprompi_bench/sync/clock_sync/clocks/MPIClock.cpp
@@ -11,7 +11,7 @@ MPIClock::~MPIClock() {
 }
 
 double MPIClock::get_time(void) {
-  return MPI_Wtime();
+  return PMPI_Wtime();
 }
 
 bool MPIClock::is_base_clock() {
diff --git a/src/reprompi_bench/sync/time_measurement.c b/src/reprompi_bench/sync/time_measurement.c
index 2fac90f..7005445 100644
--- a/src/reprompi_bench/sync/time_measurement.c
+++ b/src/reprompi_bench/sync/time_measurement.c
@@ -72,7 +72,7 @@ inline double get_time(void) {
     wtime = (double)(ts.tv_nsec) / 1.0e+9 + ts.tv_sec;
     return wtime;
 #else
-    return MPI_Wtime();
+    return PMPI_Wtime();
 #endif
 }
 
-- 
2.38.0


From ddb4510ef54a74b5f9c7a19fd276baf54fb3a219 Mon Sep 17 00:00:00 2001
From: Sascha Hunold <sascha@hunoldscience.net>
Date: Thu, 1 Sep 2022 14:19:11 +0200
Subject: [PATCH 3/5] cleanup test cases

---
 src/test/CMakeLists.txt                   |  19 +-
 src/test/test_output/CMakeLists.txt       |  19 --
 src/test/test_output/test_files/test1.txt |  16 --
 src/test/test_output/test_output_types.c  | 222 ----------------
 src/test/testbench.c                      | 308 ----------------------
 src/test/testbench.h                      |  57 ----
 6 files changed, 2 insertions(+), 639 deletions(-)
 delete mode 100644 src/test/test_output/CMakeLists.txt
 delete mode 100644 src/test/test_output/test_files/test1.txt
 delete mode 100644 src/test/test_output/test_output_types.c
 delete mode 100644 src/test/testbench.c
 delete mode 100755 src/test/testbench.h

diff --git a/src/test/CMakeLists.txt b/src/test/CMakeLists.txt
index 72a433c..79eba5e 100644
--- a/src/test/CMakeLists.txt
+++ b/src/test/CMakeLists.txt
@@ -1,26 +1,11 @@
 
 
-add_executable(testbench
-testbench.c
-${COLL_OPS_SRC_FILES}
-)
-TARGET_LINK_LIBRARIES(testbench ${COMMON_LIBRARIES} )
-
 
 add_executable(test_dict
 test_global_dict.c
 ${SRC_DIR}/reprompi_bench/misc.c
 ${SRC_DIR}/reprompi_bench/utils/keyvalue_store.c
 )
-TARGET_LINK_LIBRARIES(testbench ${COMMON_LIBRARIES} )
-
-
-
-if(COMPILE_BENCH_TESTS)
-    SET(MY_COMPILE_FLAGS "${MY_COMPILE_FLAGS} -DCOMPILE_BENCH_TESTS")
-endif()
-
-SET_TARGET_PROPERTIES(testbench PROPERTIES COMPILE_FLAGS "${MY_COMPILE_FLAGS}")
-
+TARGET_LINK_LIBRARIES(test_dict ${COMMON_LIBRARIES} )
 
-ADD_SUBDIRECTORY(${SRC_DIR}/test/test_output)
+#ADD_SUBDIRECTORY(${SRC_DIR}/test/test_output)
diff --git a/src/test/test_output/CMakeLists.txt b/src/test/test_output/CMakeLists.txt
deleted file mode 100644
index da8cc7a..0000000
--- a/src/test/test_output/CMakeLists.txt
+++ /dev/null
@@ -1,19 +0,0 @@
-
-add_executable(test_output
-${SRC_DIR}/test/test_output/test_output_types.c
-${SRC_DIR}/reprompi_bench/output_management/results_output.c
-${SRC_DIR}/reprompi_bench/output_management/runtimes_computation.c
-
-${SRC_DIR}/reprompi_bench/misc.c
-${COMMON_OPTION_PARSER_SRC_FILES}
-${SRC_DIR}/reprompi_bench/option_parser/parse_options.c
-${COLL_OPS_SRC_FILES}
-)
-
-
-TARGET_LINK_LIBRARIES(test_output ${COMMON_LIBRARIES} )
-
-
-
-
-
diff --git a/src/test/test_output/test_files/test1.txt b/src/test/test_output/test_files/test1.txt
deleted file mode 100644
index 9671c82..0000000
--- a/src/test/test_output/test_files/test1.txt
+++ /dev/null
@@ -1,16 +0,0 @@
-3 5
-1 2 1 2 0
-3 4 3 4 0
-5 6 5 6 0
-7 8 7 8 0
-9 10 9 10 0 
-1 2 1 2 0
-3 4 3 4 0
-5 6 5 6 0
-7 8 7 8 0
-9 10 9 10 0 
-1 2 1 2 0
-3 4 3 4 0
-5 6 5 6 0
-7 8 7 8 0
-9 10 9 10 0
\ No newline at end of file
diff --git a/src/test/test_output/test_output_types.c b/src/test/test_output/test_output_types.c
deleted file mode 100644
index c261ddd..0000000
--- a/src/test/test_output/test_output_types.c
+++ /dev/null
@@ -1,222 +0,0 @@
-/*  ReproMPI Benchmark
- *
- *  Copyright 2015 Alexandra Carpen-Amarie, Sascha Hunold
- Research Group for Parallel Computing
- Faculty of Informatics
- Vienna University of Technology, Austria
-
- <license>
- This program is free software: you can redistribute it and/or modify
- it under the terms of the GNU General Public License as published by
- the Free Software Foundation, either version 2 of the License, or
- (at your option) any later version.
-
- This program is distributed in the hope that it will be useful,
- but WITHOUT ANY WARRANTY; without even the implied warranty of
- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- GNU General Public License for more details.
-
- You should have received a copy of the GNU General Public License
- along with this program.  If not, see <http://www.gnu.org/licenses/>.
- </license>
- */
-
-#include <stdio.h>
-#include <string.h>
-#include <stdlib.h>
-#include <time.h>
-#include <math.h>
-#include "mpi.h"
-
-#include "reprompi_bench/output_management/runtimes_computation.h"
-#include "reprompi_bench/output_management/results_output.h"
-#include "reprompi_bench/option_parser/parse_timing_options.h"
-
-static const int OUTPUT_ROOT_PROC = 0;
-
-typedef struct bench_results {
-  double* local_start_time;
-  double* local_end_time;
-  double* global_start_time;
-  double* global_end_time;
-  int* errorcodes;
-  int nrep;
-} bench_results_t;
-
-static bench_results_t bench_results;
-static const double epsilon = 1e-10;
-
-static double mock_get_global_time(double local_time) {
-  int i, my_rank;
-  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
-
-  for (i=0; i<bench_results.nrep; i++) {
-    if (fabs(local_time - bench_results.local_start_time[i]) < epsilon ) {   // the provided local_time is a start timestamp
-        return bench_results.global_start_time[i];
-    }
-    if (fabs(local_time - bench_results.local_end_time[i]) < epsilon ) {   // the provided local_time is an end timestamp
-        return bench_results.global_end_time[i];
-    }
-  }
-
-  fprintf(stderr, "ERROR: Cannot identify a global time for the provided timestamp: rank=%d local_time=%10.9f\n", my_rank, local_time);
-  MPI_Finalize();
-  exit(0);
-}
-
-static int* mock_get_errorcodes(void) {
-  return bench_results.errorcodes;
-}
-
-static void read_input_file(char* file_name, bench_results_t* bench_res) {
-  FILE* file;
-  int result = 0;
-  int expected_result = 0;
-  int i;
-  int nprocs, p, my_rank, input_procs;
-
-  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
-  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
-
-  file = fopen(file_name, "r");
-  if (file) {
-    // the file contains data for input_procs processes, with nrep_p repetitions for each of them
-    result = fscanf(file, "%d %d", &input_procs, &(bench_res->nrep));
-    expected_result = 2;
-
-    if (result != expected_result) /* incorrectly formatted file */
-    {
-      fprintf(stderr, "ERROR: Incorrectly formatted input file: %s\n", file_name);
-      MPI_Finalize();
-      exit(0);
-    }
-
-    bench_res->local_start_time = (double*) calloc(bench_res->nrep, sizeof(double));
-    bench_res->local_end_time = (double*) calloc(bench_res->nrep, sizeof(double));
-    bench_res->global_start_time = (double*) calloc(bench_res->nrep, sizeof(double));
-    bench_res->global_end_time = (double*) calloc(bench_res->nrep, sizeof(double));
-
-    bench_res->errorcodes = (int*) calloc(bench_res->nrep, sizeof(int));
-
-    for (p = 0; p < nprocs; p++) {  // read data for all process, but only store
-                                    // the timestamps corresponding to the current process
-      for (i = 0; i < bench_res->nrep; i++) {
-        double t1, t2, tg1, tg2;
-        int e1;
-
-        result = fscanf(file, "%lf %lf %lf %lf %d", &t1, &t2, &tg1, &tg2, &e1);
-
-        /* number of job parameters: tstart[i] tend[i] errorcode*/
-        expected_result = 5;
-        if (result != expected_result) /* incorrectly formatted file */
-        {
-          fprintf(stderr, "ERROR: Incorrectly formatted input file: %s\n", file_name);
-          MPI_Finalize();
-          exit(0);
-        }
-
-        if (p == 0 || p == my_rank) { // store data corresponding to the current rank
-                                      // (or to process 0 if not all processes have data in the input file)
-          bench_res->local_start_time[i] = t1;
-          bench_res->local_end_time[i] = t2;
-          bench_res->global_start_time[i] = tg1;
-          bench_res->global_end_time[i] = tg2;
-          bench_res->errorcodes[i] = e1;
-        }
-      }
-      if (p == input_procs - 1) { // no data left in the file
-        if (p < my_rank) {
-          fprintf(stderr, "WARNING: No data for process %d in the input file. Using the data specified for process 0\n", my_rank);
-          fflush(stderr);
-        }
-        break;
-      }
-    }
-    fclose(file);
-  } else {
-    fprintf(stderr, "ERROR: Cannot open input file: %s\n", file_name);
-    MPI_Finalize();
-    exit(0);
-  }
-}
-
-int main(int argc, char* argv[]) {
-  int my_rank, procs;
-  reprompib_options_t opts;
-  reprompib_sync_module_t clock_sync;
-  reprompib_proc_sync_module_t proc_sync;
-  reprompib_timing_method_t runtime_type;
-  FILE* f = stdout;
-  job_t job;
-  int summarize;
-  char* input_file;
-  reprompib_bench_print_info_t print_info;
-
-  /* start up MPI
-   *
-   * */
-  MPI_Init(&argc, &argv);
-  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
-  MPI_Comm_size(MPI_COMM_WORLD, &procs);
-
-  if (argc < 6) {
-    if (my_rank == OUTPUT_ROOT_PROC) {
-      printf("USAGE: mpirun -np 4 %s proc_sync_type runtime_type verbose summarize input_file\n", argv[0]);
-      printf("\tArguments:\n");
-      printf("\tproc_sync_type = {0,1}, 0=window-based, 1=barrier\n");
-      printf("\truntime_type = [0-2], 0=local_times, 1=global_times, 2=local_avg(osu)\n");
-      printf("\tverbose = {0,1}\n");
-      printf("\tsummarize = {0,1}\n");
-    }
-    MPI_Finalize();
-    exit(0);
-  }
-
-  summarize = atoi(argv[4]);
-  input_file = strdup(argv[5]);
-  read_input_file(input_file, &bench_results);
-
-  // init job
-  job.n_rep = bench_results.nrep;
-  job.msize = 1;
-  job.count = 1;
-  job.call_index = 0;
-
-  // init other options
-  opts.n_rep = job.n_rep;
-  opts.print_summary_methods = 0;
-  opts.verbose = atoi(argv[3]);
-
-
-  runtime_type = atoi(argv[2]);
-
-  // init sync module
-  clock_sync.name = "test";
-
-  clock_sync.get_global_time = mock_get_global_time;
-  proc_sync.get_errorcodes = mock_get_errorcodes;
-  proc_sync.procsync = atoi(argv[1]);;
-
-  print_info.clock_sync = &clock_sync;
-  print_info.proc_sync = &proc_sync;
-  print_info.timing_method = runtime_type;
-  if (summarize == 0) {
-    print_measurement_results(f, job, bench_results.local_start_time, bench_results.local_end_time,
-        &print_info, &opts);
-  }
-  else {
-    opts.print_summary_methods = 0xFF;
-    print_summary(stdout, job, bench_results.local_start_time, bench_results.local_end_time,
-        &print_info, &opts);
-  }
-
-  free(bench_results.local_start_time);
-  free(bench_results.local_end_time);
-  free(bench_results.global_start_time);
-  free(bench_results.global_end_time);
-  free(bench_results.errorcodes);
-  /* shut down MPI */
-  MPI_Finalize();
-
-  return 0;
-}
diff --git a/src/test/testbench.c b/src/test/testbench.c
deleted file mode 100644
index 2f4085f..0000000
--- a/src/test/testbench.c
+++ /dev/null
@@ -1,308 +0,0 @@
-/*  ReproMPI Benchmark
- *
- *  Copyright 2015 Alexandra Carpen-Amarie, Sascha Hunold
-    Research Group for Parallel Computing
-    Faculty of Informatics
-    Vienna University of Technology, Austria
-
-<license>
-    This program is free software: you can redistribute it and/or modify
-    it under the terms of the GNU General Public License as published by
-    the Free Software Foundation, either version 2 of the License, or
-    (at your option) any later version.
-
-    This program is distributed in the hope that it will be useful,
-    but WITHOUT ANY WARRANTY; without even the implied warranty of
-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-    GNU General Public License for more details.
-
-    You should have received a copy of the GNU General Public License
-    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-</license>
- */
-
-#include <stdio.h>
-#include <string.h>
-#include <stdlib.h>
-#include <time.h>
-#include "mpi.h"
-
-//#include "parse_options.h"
-#include "../collective_ops/collectives.h"
-#include "testbench.h"
-
-typedef double test_type;
-
-
-void set_buffer_sequentially(const test_type start_val, const int n_elems, char* buffer) {
-    int i;
-    test_type val = start_val;
-    test_type *buff;
-
-    buff = (test_type*)buffer;
-    for (i=0; i< n_elems; i++) {
-        buff[i] = val++;
-    }
-}
-
-
-void set_buffer_random(const int n_elems, char* buffer) {
-    int i;
-    test_type *buff;
-
-    buff = (test_type*)buffer;
-    for (i=0; i< n_elems; i++) {
-        buff[i] = rand();
-    }
-}
-
-
-void set_buffer_sequentially_char(const char start_val, const int n_elems, char* buffer) {
-    int i;
-    char val = start_val;
-    //int *buff;
-
-    //buff = (int*)buffer;
-    for (i=0; i< n_elems; i++) {
-        buffer[i] = val++;
-    }
-}
-
-void set_buffer_const(const test_type val, const int n_elems, char* buffer) {
-    int i;
-    test_type *buff;
-
-    buff = (test_type*)buffer;
-    for (i=0; i< n_elems; i++) {
-        buff[i] = val;
-    }
-}
-
-
-void collect_buffers(const collective_params_t params, char* send_buffer, char* recv_buffer) {
-
-    // gather measurement results
-    MPI_Gather(params.sbuf,  params.scount * params.datatype_extent, MPI_CHAR,
-            send_buffer,  params.scount * params.datatype_extent, MPI_CHAR,
-            0, MPI_COMM_WORLD);
-    MPI_Gather(params.rbuf,  params.rcount * params.datatype_extent, MPI_CHAR,
-            recv_buffer,  params.rcount * params.datatype_extent, MPI_CHAR,
-            0, MPI_COMM_WORLD);
-}
-
-
-void print_buffers(char coll1[], char coll2[], test_type* buffer1, test_type* buffer2, int count) {
-/*
-    int my_rank;
-    int i;
-    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
-
-    if (my_rank == 0) {
-        printf("%s\n", coll1);
-        for (i=0; i< count; i++) {
-            printf ("%lf ", (double)buffer1[i] );
-        }
-        printf("\n");
-
-        printf("%s\n", coll2);
-        for (i=0; i< count; i++) {
-            printf ("%lf ", (double)buffer2[i] );
-        }
-        printf("\n");
-    }*/
-
-}
-
-
-int identical(test_type* buffer1, test_type* buffer2, int count) {
-    int i;
-    int identical = 1;
-
-    for (i=0; i< count; i++) {
-        if (buffer1[i] != buffer2[i]) {
-            identical = 0;
-        }
-    }
-
-    return identical;
-}
-
-
-void check_results(char coll1[], char coll2[],
-        collective_params_t coll_params,
-        collective_params_t mockup_params,
-        int check_only_at_root) {
-
-    int my_rank, p;
-    int error = 0;
-    test_type *send_buffer, *mockup_send_buffer;
-    test_type *recv_buffer, *mockup_recv_buffer;
-
-    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
-
-    // gather send and receive buffers from all processes
-    send_buffer = (test_type*)malloc(coll_params.nprocs* coll_params.scount * coll_params.datatype_extent);
-    mockup_send_buffer = (test_type*)malloc(mockup_params.nprocs* mockup_params.scount * mockup_params.datatype_extent);
-
-    recv_buffer = (test_type*)malloc(coll_params.nprocs* coll_params.rcount * coll_params.datatype_extent);
-    mockup_recv_buffer = (test_type*)malloc(mockup_params.nprocs* mockup_params.rcount * mockup_params.datatype_extent);
-
-    collect_buffers(coll_params, (char*)send_buffer, (char*)recv_buffer);
-    collect_buffers(mockup_params, (char*)mockup_send_buffer, (char*)mockup_recv_buffer);
-
-    if (my_rank == 0) {
-        printf ("----------------------------------------\n");
-        printf ("---------------- Comparing functions %s and %s\n", coll1, coll2);
-
-        if (check_only_at_root) {
-            p = coll_params.root;
-            print_buffers(coll1, coll2,
-                    recv_buffer + p*coll_params.rcount,
-                    mockup_recv_buffer + p*coll_params.rcount,
-                    coll_params.rcount);
-            error = (!identical(recv_buffer + p*coll_params.rcount,
-                    mockup_recv_buffer + p*coll_params.rcount,
-                    coll_params.rcount));
-        }
-        else {
-            if (strcmp(coll1, "MPI_Bcast")){
-
-                for (p=0; p<coll_params.nprocs; p++) {
-                    printf ("=========== Process %d\n", p);
-
-                    print_buffers(coll1, coll2,
-                            recv_buffer + p*coll_params.rcount,
-                            mockup_recv_buffer + p*coll_params.rcount,
-                            coll_params.rcount);
-                }
-                error = (!identical(recv_buffer, mockup_recv_buffer, coll_params.nprocs * coll_params.rcount));
-            }
-            else {
-                // for Bcast check source buffers
-                for (p=0; p<coll_params.nprocs; p++) {
-                    printf ("=========== Process %d\n", p);
-
-                    print_buffers(coll1, coll2,
-                            send_buffer + p*coll_params.scount,
-                            mockup_send_buffer + p*coll_params.scount,
-                            coll_params.scount);
-                }
-                error = (!identical(send_buffer, mockup_send_buffer, coll_params.nprocs * coll_params.scount));
-
-            }
-
-        }
-        if (error) {
-            printf ("****************\n**************** TEST FAILED for %s and %s\n", coll1, coll2);
-            printf("****************\n****************\n\n");
-        }
-        else {
-            printf ("---- Test passed.\n\n");
-        }
-    }
-}
-
-
-void test_collective(basic_collective_params_t basic_coll_info,
-        long count, int coll_index, int mockup_index) {
-
-    long n_elems, i;
-    int check_only_at_root = 0;
-
-    collective_params_t coll_params, mockup_params;
-
-    // initialize operations
-    collective_calls[coll_index].initialize_data(basic_coll_info, count, &coll_params);
-    collective_calls[mockup_index].initialize_data(basic_coll_info, count, &mockup_params);
-
-    // setup buffers
-    n_elems = coll_params.scount;
-    set_buffer_random(n_elems, coll_params.sbuf);
-    for (i=0;i<n_elems;i++) {
-        ((test_type*)mockup_params.sbuf)[i] = ((test_type*)coll_params.sbuf)[i];
-    }
-
-    // execute collective op
-    collective_calls[coll_index].collective_call(&coll_params);
-    collective_calls[mockup_index].collective_call(&mockup_params);
-
-    if (coll_index == MPI_GATHER || coll_index == MPI_REDUCE) {
-        check_only_at_root = 1;
-    }
-    check_results(get_call_from_index(coll_index), get_call_from_index(mockup_index),
-            coll_params, mockup_params, check_only_at_root);
-
-
-
-    // cleanup data
-    collective_calls[coll_index].cleanup_data(&coll_params);
-    collective_calls[mockup_index].cleanup_data(&mockup_params);
-
-}
-
-
-
-
-int main(int argc, char* argv[]) {
-    int nprocs;
-    basic_collective_params_t basic_coll_info;
-    long count = 100;
-
-
-    srand(1000);
-
-
-    if (argc > 1) {
-        count = atol(argv[1]);
-    }
-
-    /* start up MPI
-     *
-     * */
-    MPI_Init(&argc, &argv);
-
-
-    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
-
-    basic_coll_info.datatype = MPI_DOUBLE;
-    basic_coll_info.op = MPI_SUM;
-    basic_coll_info.root = 0;
-    basic_coll_info.nprocs = nprocs;
-
-    test_collective(basic_coll_info, count, MPI_ALLGATHER, GL_ALLGATHER_AS_ALLREDUCE);
-    test_collective(basic_coll_info, count, MPI_ALLGATHER, GL_ALLGATHER_AS_ALLTOALL);
-    test_collective(basic_coll_info, count, MPI_ALLGATHER, GL_ALLGATHER_AS_GATHERBCAST);
-
-    test_collective(basic_coll_info, count, MPI_ALLREDUCE, GL_ALLREDUCE_AS_REDUCEBCAST);
-    test_collective(basic_coll_info, count, MPI_ALLREDUCE, GL_ALLREDUCE_AS_REDUCESCATTERALLGATHERV);
-
-    test_collective(basic_coll_info, count, MPI_BCAST, GL_BCAST_AS_SCATTERALLGATHER);
-
-    test_collective(basic_coll_info, count, MPI_GATHER, GL_GATHER_AS_ALLGATHER);
-    test_collective(basic_coll_info, count, MPI_GATHER, GL_GATHER_AS_REDUCE);
-
-    test_collective(basic_coll_info, count, MPI_REDUCE, GL_REDUCE_AS_ALLREDUCE);
-
-    test_collective(basic_coll_info, count, MPI_REDUCE, GL_REDUCE_AS_REDUCESCATTERGATHERV);
-
-    test_collective(basic_coll_info, count, MPI_REDUCE_SCATTER, GL_REDUCESCATTER_AS_ALLREDUCE);
-    test_collective(basic_coll_info, count, MPI_REDUCE_SCATTER, GL_REDUCESCATTER_AS_REDUCESCATTERV);
-    test_collective(basic_coll_info, count, MPI_REDUCE_SCATTER_BLOCK, GL_REDUCESCATTERBLOCK_AS_REDUCESCATTER);
-
-    test_collective(basic_coll_info, count, MPI_SCAN, GL_SCAN_AS_EXSCANREDUCELOCAL);
-
-    test_collective(basic_coll_info, count, MPI_SCATTER, GL_SCATTER_AS_BCAST);
-
-    if (count % nprocs == 0) {  // only works if the number of processes is a divisor of count
-        test_collective(basic_coll_info, count, MPI_REDUCE, GL_REDUCE_AS_REDUCESCATTERBLOCKGATHER);
-        test_collective(basic_coll_info, count, MPI_ALLREDUCE, GL_ALLREDUCE_AS_REDUCESCATTERBLOCKALLGATHER);
-    }
-    else {
-
-    }
-
-    /* shut down MPI */
-    MPI_Finalize();
-
-    return 0;
-}
diff --git a/src/test/testbench.h b/src/test/testbench.h
deleted file mode 100755
index 7701151..0000000
--- a/src/test/testbench.h
+++ /dev/null
@@ -1,57 +0,0 @@
-/*  ReproMPI Benchmark
- *
- *  Copyright 2015 Alexandra Carpen-Amarie, Sascha Hunold
-    Research Group for Parallel Computing
-    Faculty of Informatics
-    Vienna University of Technology, Austria
-
-<license>
-    This program is free software: you can redistribute it and/or modify
-    it under the terms of the GNU General Public License as published by
-    the Free Software Foundation, either version 2 of the License, or
-    (at your option) any later version.
-
-    This program is distributed in the hope that it will be useful,
-    but WITHOUT ANY WARRANTY; without even the implied warranty of
-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-    GNU General Public License for more details.
-
-    You should have received a copy of the GNU General Public License
-    along with this program.  If not, see <http://www.gnu.org/licenses/>.
-</license>
-*/
-
-#ifndef TESTBENCH_H_
-#define TESTBENCH_H_
-
-enum tests {
-    TEST_SCATTER_AS_BCAST = 0,
-    TEST_ALLGATHER_AS_ALLREDUCE,
-    TEST_GATHER_AS_REDUCE,
-    TEST_ALLGATHER_GATHERBCAST,
-    TEST_BCAST_AS_SCATTERALLGATHER,
-    TEST_ALLREDUCE_AS_REDUCEBCAST
-};
-
-static const int N_TESTS = 6;
-
-
-//int test_scatter_as_bcast(int nprocs, test_params_t params);
-
-/*
-int (*test_functions[])(int nprocs, test_params_t params) = {
-        [TEST_SCATTER_AS_BCAST] = &test_scatter_as_bcast,
-        [TEST_ALLGATHER_AS_ALLREDUCE] = &test_scatter_as_bcast,
-        [TEST_GATHER_AS_REDUCE] = &test_scatter_as_bcast,
-        [TEST_ALLGATHER_GATHERBCAST] = &test_scatter_as_bcast,
-        [TEST_BCAST_AS_SCATTERALLGATHER] = &test_scatter_as_bcast,
-        [TEST_ALLREDUCE_AS_REDUCEBCAST] = &test_scatter_as_bcast
-};
-*/
-
-
-
-#endif /* TESTBENCH_H_ */
-
-
-
-- 
2.38.0


From 8df8b3c9ef34b6ccbaf9378df060552155fe7672 Mon Sep 17 00:00:00 2001
From: Sascha Hunold <sascha@hunoldscience.net>
Date: Fri, 2 Sep 2022 13:44:06 +0200
Subject: [PATCH 4/5] updated README

---
 README.md  | 206 +++++++++++++++++++++++++++++++++++
 README.org | 314 -----------------------------------------------------
 2 files changed, 206 insertions(+), 314 deletions(-)
 create mode 100644 README.md
 delete mode 100644 README.org

diff --git a/README.md b/README.md
new file mode 100644
index 0000000..1ca0f71
--- /dev/null
+++ b/README.md
@@ -0,0 +1,206 @@
+# ReproMPI Benchmark (Development Version)
+
+
+## Introduction
+
+The ReproMPI Benchmark is a tool designed to accurately measure the
+run-time of MPI blocking collective operations.  It provides multiple
+process synchronization methods and a flexible mechanism for
+predicting the number of measurements that are sufficient to obtain
+statistically sound results.
+
+
+## Installation
+
+- Prerequisites
+  - an MPI library 
+  - CMake (version >= 3.0)  
+  - GSL libraries 
+
+## Basic installation
+
+```
+  cd $BENCHMARK_PATH
+  ./cmake .
+  make
+```
+
+For specific configuration options check the *Benchmark Configuration* section.
+
+## Running the ReproMPI Benchmark
+
+The ReproMPI code is designed to serve two specific purposes:
+
+## Benchmarking of MPI collective calls
+The most common usage scenario of the benchmark is to specify an MPI
+collective function to be benchmarked, a (list of) message sizes and
+the *number of measurement repetitions* for each test, as in the
+following example.
+
+```
+mpirun -np 4 ./bin/mpibenchmark --calls-list=MPI_Bcast,MPI_Allgather 
+             --msizes-list=8,1024,2048  --nrep=10
+```
+
+
+
+## Command-line Options
+
+### Common Options
+
+  - `-h` print help
+  - `-v` print run-times measured for each process
+  - `--msizes-list`<values>= list of comma-separated message sizes in
+    Bytes, e.g., `--msizes-list=10,1024`
+  - `--msize-interval=min=<min>,max=<max>,step=<step>` list of power
+    of 2 message sizes as an interval between $2^{min}$ and $2^{max}$,
+    with $2^{step}$ distance between values, e.g., 
+    `--msize-interval=min=1,max=4,step=1`
+  - `--calls-list=<args>` list of comma-separated MPI calls to be
+    benchmarked, e.g., `--calls-list=MPI_Bcast,MPI_Allgather`
+  - `--root-proc=<process_id>` root node for collective operations     
+  - `--operation=<mpi_op>` MPI operation applied by collective
+    operations (where applicable), e.g., `--operation=MPI_BOR`.
+    
+    Supported operations: MPI_BOR, MPI_BAND, MPI_LOR, MPI_LAND,
+    MPI_MIN, MPI_MAX, MPI_SUM, MPI_PROD 
+  - `--datatype=<mpi_type>` MPI datatype used by collective
+    operations, e.g., `--datatype=MPI_CHAR`.
+
+    Supported datatypes: `MPI_CHAR`, `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`
+  - `--shuffle-jobs` shuffle experiments before running the benchmark
+  - `--params=k1:v1,k2:v2` list of comma-separated =key:value= pairs
+    to be printed in the benchmark output.
+  - `-f | --input-file=<path>` input file containing the list of
+    benchmarking jobs (tuples of MPI function, message size, number of
+    repetitions). It replaces all the other common options.
+  
+  
+### Options Related to the Window-based Synchronization
+
+  - `--window-size=<win>` window size in microseconds for Window-based synchronization
+
+
+### Specific Options for the ReproMPI Benchmark
+
+  - `--nrep=<nrep>` set number of experiment repetitions
+  - `--summary=<args>` list of comma-separated data summarizing
+    methods (mean, median, min, max), e.g., `--summary=mean,max`
+
+
+## Supported Collective Operations:
+### MPI Collectives
+
+  - `MPI_Allgather`
+  - `MPI_Allreduce`
+  - `MPI_Alltoall`
+  - `MPI_Barrier`
+  - `MPI_Bcast`
+  - `MPI_Exscan`
+  - `MPI_Gather`
+  - `MPI_Reduce`
+  - `MPI_Reduce_scatter`
+  - `MPI_Reduce_scatter_block`
+  - `MPI_Scan`
+  - `MPI_Scatter`
+
+### Mockup Functions of Various MPI Collectives
+  TODO
+
+    
+
+## Process Synchronization Methods
+
+### MPI_Barrier
+This is the default synchronization method enabled for the benchmark.
+
+### Dissemination Barrier
+To benchmark collective operations acorss multiple MPI libraries using
+the same barrier implementation, the benchmark provides a
+dissemination barrier that can replace the default MPI_Barrier to
+synchronize processes.
+
+To enable the dissemination barrier, the following flag has to be set
+before compiling the benchmark (e.g., using the =ccmake= command).
+
+> `ENABLE_BENCHMARK_BARRIER`
+
+Both barrier-based synchronization methods can alternatively use a
+double barrier before each measurement.
+
+> `ENABLE_DOUBLE_BARRIER`
+
+
+### Window-based Synchronization
+
+The ReproMPI benchmark implements a window-based process
+synchronization mechanism, which estimates the clock offset/drift of
+each process relative to a reference process and then uses the
+obtained global clocks to synchronize processes before each
+measurement and to compute run-times.
+
+
+### Timing procedure
+  
+  The MPI operation run-time is computed in a different manner
+  depending on the selected clock synchronization method. If global
+  clocks are available, the run-times are computed as the difference
+  between the largest exit time and the first start time among all
+  processes.
+
+  If a barrier-based synchronization is used, the run-time of an MPI
+  call is computed as the largest local run-time across all processes.
+
+  However, the timing proceduce that relies on global clocks can be
+  used in combination with a barrier-based synchronization when the
+  following flag is enabled:
+
+
+### Clock resolution
+
+The =MPI_Wtime= cll is used by default to obtain the current time.
+To obtain accurate measurements of short time intervals, the benchmark
+can rely on the high resolution =RDTSC/RDTSCP= instructions (if they are
+available on the test machines) by setting on of the following flags:
+```
+ENABLE_RDTSC
+ENABLE_RDTSCP
+```
+
+Additionally, setting the clock frequency of the CPU is required to
+obtain accurate measurements:
+```
+FREQUENCY_MHZ                    2300
+```
+
+The clock frequency can also be automatically estimated (as done by
+the NetGauge tool) by enabling the following variable:
+```
+CALIBRATE_RDTSC
+```
+
+However, this method reduces the results accuracy and we advise to
+manually set the highest CPU frequency instead. More details about
+the usage of =RDTSC=-based timers can be found in our research
+report.
+
+## List of Compilation Flags
+
+This is the full list of compilation flags that can be used to control
+all the previously detailed configuration parameters.
+
+```
+ CALIBRATE_RDTSC                  OFF   
+ COMPILE_BENCH_TESTS              OFF                 
+ COMPILE_SANITY_CHECK_TESTS       OFF               
+ ENABLE_BENCHMARK_BARRIER         OFF             
+ ENABLE_DOUBLE_BARRIER            OFF             
+ ENABLE_GLOBAL_TIMES              OFF             
+ ENABLE_LOGP_SYNC                 OFF             
+ ENABLE_RDTSC                     OFF             
+ ENABLE_RDTSCP                    OFF           
+ ENABLE_WINDOWSYNC_HCA            OFF            
+ ENABLE_WINDOWSYNC_JK             OFF        
+ ENABLE_WINDOWSYNC_SK             OFF      
+ FREQUENCY_MHZ                    2300    
+```
diff --git a/README.org b/README.org
deleted file mode 100644
index d8d2c68..0000000
--- a/README.org
+++ /dev/null
@@ -1,314 +0,0 @@
-#  -*- mode: org; -*-
-
-#+TITLE:       ReproMPI Benchmark
-#+AUTHOR:      
-#+EMAIL:       
-
-#+OPTIONS: ^:nil toc:nil <:nil
-
-#+LaTeX_CLASS_OPTIONS: [a4paper]
-#+LaTeX_CLASS_OPTIONS: [11pt]
-
-#+LATEX_HEADER: \usepackage{bibentry}
-#+LATEX_HEADER: \nobibliography*
-#+LATEX_HEADER: \usepackage{listings}
-
-
-* Introduction
-
-The ReproMPI Benchmark is a tool designed to accurately measure the
-run-time of MPI blocking collective operations.  It provides multiple
-process synchronization methods and a flexible mechanism for
-predicting the number of measurements that are sufficient to obtain
-statistically sound results.
-
-
-* Installation
-
-** Prerequisites
-  - an MPI library 
-  - CMake (version >= 2.6)  
-  - GSL libraries 
-
-** Basic installation
-
-#+BEGIN_EXAMPLE
-  cd $BENCHMARK_PATH
-  ./cmake .
-  make
-
-#+END_EXAMPLE
-
-For specific configuration options check the *Benchmark Configuration* section.
-
-* Running the ReproMPI Benchmark
-
-The ReproMPI code is designed to serve two specific purposes:
-
-** Benchmarking of MPI collective calls
-The most common usage scenario of the benchmark is to specify an MPI
-collective function to be benchmarked, a (list of) message sizes and
-the *number of measurement repetitions* for each test, as in the
-following example.
-
-#+BEGIN_EXAMPLE
-mpirun -np 4 ./bin/mpibenchmark --calls-list=MPI_Bcast,MPI_Allgather 
-             --msizes-list=8,1024,2048  --nrep=10
-#+END_EXAMPLE
-
-** Estimating the number of measurement iterations
-In this scenario, the user can generate an estimation of the number of
-measurements required for a stable result with respect to one or
-multiple prediction methods.
-
-More details about the various methods that are supported and their
-usage can be found in:
-- S. Hunold, A. Carpen-Amarie, F.D. Lübbe and J.L. Träff, "Automatic
-   Verification of Self-Consistent MPI Performance Guidelines",
-   EuroPar (2016)
-
-This is an example of how to perform such an estimation:
-
-#+BEGIN_EXAMPLE
-mpirun -np 4 ./bin/mpibenchmarkPredNreps --calls-list=MPI_Bcast,MPI_Allgather 
-              --msizes-list=8,1024,2048 --rep-prediction=min=1,max=200,step=5
-#+END_EXAMPLE
-
-
-** Command-line Options
-
-*** Common Options
-
-  - =-h= print help
-  - =-v= print run-times measured for each process
-  - =--msizes-list=<values>= list of comma-separated message sizes in
-    Bytes, e.g., =--msizes-list=10,1024=
-  - =--msize-interval=min=<min>,max=<max>,step=<step>= list of power
-    of 2 message sizes as an interval between 2^min and 2^max,
-    with 2^step distance between values, e.g., 
-    =--msize-interval=min=1,max=4,step=1=
-  - =--calls-list=<args>= list of comma-separated MPI calls to be
-    benchmarked, e.g., =--calls-list=MPI_Bcast,MPI_Allgather=
-  - =--root-proc=<process_id>= root node for collective operations     
-  - =--operation=<mpi_op>= MPI operation applied by collective
-    operations (where applicable), e.g., =--operation=MPI_BOR=.
-    
-    Supported operations: MPI_BOR, MPI_BAND, MPI_LOR, MPI_LAND,
-    MPI_MIN, MPI_MAX, MPI_SUM, MPI_PROD 
-  - =--datatype=<mpi_type>= MPI datatype used by collective
-    operations, e.g., =--datatype=MPI_CHAR=.
-
-    Supported datatypes: MPI_CHAR, MPI_INT, MPI_FLOAT, MPI_DOUBLE
-  - =--shuffle-jobs= shuffle experiments before running the benchmark
-  - =--params=k1:v1,k2:v2= list of comma-separated =key:value= pairs
-    to be printed in the benchmark output.
-  - =-f | --input-file=<path>= input file containing the list of
-    benchmarking jobs (tuples of MPI function, message size, number of
-    repetitions). It replaces all the other common options.
-  
-  
-*** Options Related to the Window-based Synchronization
-
-  - =--window-size=<win>= window size in microseconds for Window-based synchronization
-
-Specific options for synchronization methods based on a linear model of the clock drift
-  - =--fitpoints=<nfit>= number of fitpoints (default: 20) 
-  - =--exchanges=<nexc>= number of exchanges (default: 10)
-
-*** Specific Options for the ReproMPI Benchmark
-
-  - =--nrep=<nrep>= set number of experiment repetitions
-  - =--summary=<args>= list of comma-separated data summarizing
-    methods (mean, median, min, max), e.g., =--summary=mean,max=
-
-*** Specific Options for Estimating the Number of Repetitions
-  - =--rep-prediction=min=<min>,max=<max>,step=<step>= set the total
-    number of repetitions to be estimated between =<min>= and =<max>=,
-    so that at each iteration i, the number of measurements (=nrep=)
-    is either =nrep(0) = <min>=, or =nrep(i) = nrep(i-1) + <step> *
-    2^(i-1)=, e.g., =--rep-prediction=min=1,max=4,step=1=
-  - =--pred-method=m1,m2= comma-separated list of prediction
-    methods, i.e., rse, cov_mean, cov_median (default: rse)
-  - =--var-thres=thres1,thres2= comma-separated list of thresholds
-    corresponding to the specified prediction methods (default: 0.01)
-  - =--var-win=win1,win2= comma-separated list of (non-zero) windows
-    corresponding to the specified prediction methods; =rse= does not
-    rely on a measurement window, however a dummy window value is
-    required in this list when multiple methods are used (default: 10)
-
-
-** Supported Collective Operations:
-*** MPI Collectives
-
-  - MPI_Allgather
-  - MPI_Allreduce
-  - MPI_Alltoall
-  - MPI_Barrier
-  - MPI_Bcast
-  - MPI_Exscan
-  - MPI_Gather
-  - MPI_Reduce
-  - MPI_Reduce_scatter
-  - MPI_Reduce_scatter_block
-  - MPI_Scan
-  - MPI_Scatter
-
-*** Mockup Functions of Various MPI Collectives
-  - GL_Allgather_as_Allreduce
-  - GL_Allgather_as_Alltoall
-  - GL_Allgather_as_GatherBcast
-  - GL_Allreduce_as_ReduceBcast
-  - GL_Allreduce_as_ReducescatterAllgather
-  - GL_Allreduce_as_ReducescatterblockAllgather
-  - GL_Bcast_as_ScatterAllgather
-  - GL_Gather_as_Allgather
-  - GL_Gather_as_Reduce
-  - GL_Reduce_as_Allreduce
-  - GL_Reduce_as_ReducescatterGather
-  - GL_Reduce_as_ReducescatterblockGather
-  - GL_Reduce_scatter_as_Allreduce
-  - GL_Reduce_scatter_as_ReduceScatterv
-  - GL_Reduce_scatter_block_as_ReduceScatter
-  - GL_Scan_as_ExscanReducelocal
-  - GL_Scatter_as_Bcast
-
-    
-* Benchmark Configuration
-
-
-** Process Synchronization Methods
-
-*** MPI_Barrier
-This is the default synchronization method enabled for the benchmark.
-
-*** Dissemination Barrier
-To benchmark collective operations acorss multiple MPI libraries using
-the same barrier implementation, the benchmark provides a
-dissemination barrier that can replace the default MPI_Barrier to
-synchronize processes.
-
-To enable the dissemination barrier, the following flag has to be set
-before compiling the benchmark (e.g., using the =ccmake= command).
-
-#+BEGIN_EXAMPLE
-ENABLE_BENCHMARK_BARRIER
-#+END_EXAMPLE
-
-Both barrier-based synchronization methods can alternatively use a
-double barrier before each measurement.
-#+BEGIN_EXAMPLE
-ENABLE_DOUBLE_BARRIER
-#+END_EXAMPLE
-
-*** Window-based Synchronization
-
-The ReproMPI benchmark implements a window-based process
-synchronization mechanism, which estimates the clock offset/drift of
-each process relative to a reference process and then uses the
-obtained global clocks to synchronize processes before each
-measurement and to compute run-times.
-
-It relies on one of the following clock
-synchronization methods:
-
-  - *HCA synchronization*: this is the clock synchronization algorithm
-    we propose in []. It computes a linear model of the clock drift of
-    each process.  The HCA method can be configured by setting the
-    following flags before compilation.
-#+BEGIN_EXAMPLE
-ENABLE_WINDOWSYNC_HCA 
-ENABLE_LOGP_SYNC
-#+END_EXAMPLE
-
-The =ENABLE_LOGP_SYNC= flag determines which variant of the HCA
-algorithm is used, i.e., either HCA1 (which computes the clock models
-in O(p) steps) or HCA2 (which requires only O(log p) rounds).
-
-  - *SKaMPI synchronization*: it implements the SKaMPI clock
-    synchronization algorithm. To enable it, set the following flag
-    before compilation.
-#+BEGIN_EXAMPLE
-ENABLE_WINDOWSYNC_SK
-#+END_EXAMPLE
-
-  - *Jones and Koenig synchronization*: it implements the clock
-    synchronization algorithm introduced by Jones and Koenig~[]. To
-    enable it, set the following flag before compilation.
-#+BEGIN_EXAMPLE
-ENABLE_WINDOWSYNC_JK
-#+END_EXAMPLE
-
-
-** Timing procedure
-  
-  The MPI operation run-time is computed in a different manner
-  depending on the selected clock synchronization method. If global
-  clocks are available, the run-times are computed as the difference
-  between the largest exit time and the first start time among all
-  processes.
-
-  If a barrier-based synchronization is used, the run-time of an MPI
-  call is computed as the largest local run-time across all processes.
-
-  However, the timing proceduce that relies on global clocks can be
-  used in combination with a barrier-based synchronization when the
-  following flag is enabled:
-#+BEGIN_EXAMPLE
-ENABLE_GLOBAL_TIMES
-#+END_EXAMPLE
-
-  More information regarding the timing procedure can be found in [].
-
-** Clock resolution
-
-  The =MPI_Wtime= cll is used by default to obtain the current time.
-To obtain accurate measurements of short time intervals, the benchmark
-can rely on the high resolution =RDTSC/RDTSCP= instructions (if they are
-available on the test machines) by setting on of the following flags:
-#+BEGIN_EXAMPLE
-ENABLE_RDTSC
-ENABLE_RDTSCP
-#+END_EXAMPLE
-
-Additionally, setting the clock frequency of the CPU is required to
-obtain accurate measurements:
-#+BEGIN_EXAMPLE
-FREQUENCY_MHZ                    2300
-#+END_EXAMPLE
-
-The clock frequency can also be automatically estimated (as done by
-the NetGauge tool) by enabling the following variable:
-#+BEGIN_EXAMPLE
-CALIBRATE_RDTSC
-#+END_EXAMPLE
-
-However, this method reduces the results accuracy and we advise to
-manually set the highest CPU frequency instead. More details about
-the usage of =RDTSC=-based timers can be found in our research
-report[].
-
-
-
-* List of Compilation Flags
-
-This is the full list of compilation flags that can be used to control
-all the previously detailed configuration parameters.
-
-#+BEGIN_EXAMPLE
- CALIBRATE_RDTSC                  OFF   
- COMPILE_BENCH_TESTS              OFF          
- COMPILE_PRED_BENCHMARK           ON                
- COMPILE_SANITY_CHECK_TESTS       OFF               
- ENABLE_BENCHMARK_BARRIER         OFF             
- ENABLE_DOUBLE_BARRIER            OFF             
- ENABLE_GLOBAL_TIMES              OFF             
- ENABLE_LOGP_SYNC                 OFF             
- ENABLE_RDTSC                     OFF             
- ENABLE_RDTSCP                    OFF           
- ENABLE_WINDOWSYNC_HCA            OFF            
- ENABLE_WINDOWSYNC_JK             OFF        
- ENABLE_WINDOWSYNC_SK             OFF      
- FREQUENCY_MHZ                    2300    
-#+END_EXAMPLE
-
-
-- 
2.38.0


From 8ed36a14f922afb06e9b565a44066eac8e40e2eb Mon Sep 17 00:00:00 2001
From: Sascha Hunold <sascha@hunoldscience.net>
Date: Fri, 2 Sep 2022 13:45:07 +0200
Subject: [PATCH 5/5] updated backticks

---
 README.md | 8 ++++++--
 1 file changed, 6 insertions(+), 2 deletions(-)

diff --git a/README.md b/README.md
index 1ca0f71..a56e401 100644
--- a/README.md
+++ b/README.md
@@ -123,12 +123,16 @@ synchronize processes.
 To enable the dissemination barrier, the following flag has to be set
 before compiling the benchmark (e.g., using the =ccmake= command).
 
-> `ENABLE_BENCHMARK_BARRIER`
+```
+ENABLE_BENCHMARK_BARRIER
+```
 
 Both barrier-based synchronization methods can alternatively use a
 double barrier before each measurement.
 
-> `ENABLE_DOUBLE_BARRIER`
+```
+ENABLE_DOUBLE_BARRIER
+```
 
 
 ### Window-based Synchronization
-- 
2.38.0

